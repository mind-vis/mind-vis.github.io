<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>CurveNet</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding
            <br /><br />
            <!-- <small>
                CVPR 2021 (Oral presentation)
            </small>
            <br /><br /> -->
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        Zijiao Chen<sup>1*</sup>
                    </li>
                    <li>
                        Jiaxin Qing<sup>2*</sup>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=sskixKkAAAAJ&hl">
                          Tiange Xiang
                        </a><sup>3</sup>
                    </li>
                    <li>
                        Wan Lin Yue<sup>1</sup>
                    </li>
                     <li>
                        <a href="https://scholar.google.com.sg/citations?user=4Z1S3_oAAAAJ&hl">
                          Juan(Helen) Zhou
                        </a><sup>1</sup>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>National University of Singapore
                    </li>
                    <li>
                        <sup>2</sup>The Chinese University of Hong Kong
                    </li>
                    <li>
                        <sup>3</sup>Standford University
                    </li>
                    <li>
                        <sup>*</sup>Equal Contribution
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2105.01288" target='_blank'>
                        <img src="./index_files/images/paper.png" height="80px"><br>
                            <h4><strong>Main Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="./supplementary.pdf" target='_blank'>
                        <img src="./index_files/images/supp.png" height="80px"><br>
                            <h4><strong>Supp. Materials</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#video">
                        <img src="index_files/images/youtube_icon_dark.png" height="80px"><br>
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                     <li>
                        <a href="#dataset">
                        <img src="./index_files/images/data.png" height="80px"><br>
                            <h4><strong>Data</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/zjc062/mind-vis">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/first_fig.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human vision and computer vision through the Brain-Computer Interface. 
                    However, due to the scarcity of data annotations and the complexity of underlying brain information, it is challenging to decode images with faithful details and meaningful semantics. 
                    In this work, we present <b>MinD-Vis</b>: Sparse <b>M</b>asked Bra<b>in</b> Modeling with <b>D</b>ouble<b>-</b>Conditioned Latent Diffusion Model for Human <b>Vis</b>ion Decoding.                   
                    Specifically, by boosting the information capacity of representations learned in a large-scale resting-state fMRI dataset, we show that our MinD-Vis framework reconstructed highly plausible images with semantically matching details from brain recordings with very few training pairs. 
                    We quantitatively benchmarked our model and the experimental results demonstrate that our method outperformed state-of-the-arts in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41%, respectively.
                    Exhaustive ablation studies are conducted to analyze our framework. 
                </p>
            </div>
        </div>

        <!-- <div class="row" id="video">
            <div class="col-md-8 col-md-offset-2">
                <h3>Video</h3><p style="color:blue;font-size:11px;">(contains audio w/ subtitles)</p>
                <div class="text-center">
                    <video id="video_id" width="100%" controls="" controlsList="nodownload">>
                        <source src="./index_files/videos/video.mp4" type="video/mp4">
                        <track label="English" kind="subtitles" srclang="en" src="./index_files/videos/captions.vtt">
                    </video>

                    <script type="text/javascript">
                        $(document).ready(function() {
                        var video = document.querySelector('#video_id'); // get the video element
                        var tracks = video.textTracks; // one for each track element
                        var track = tracks[0]; // corresponds to the first track element
                        track.mode = 'hidden';});
                    </script>

                </div>
            </div>
        </div> -->
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            A human visual decoding system that only reply on limited annotations.
                        </li>
                        <li>
                            State-of-the-art 100-way top-1 classification accuracy on GOD dataset: <b>23.9%</b>
                            <!-- State-of-the-art 100-way top-1 classification accuracy on GOD dataset: <b>23.9%</b> <i style="color:gray;font-size:8px;">w/o voting</i>, <b>94.2%</b> <i style="color:gray;font-size:8px;">w/ voting</i>. -->
                        </li>
                        <li>
                            State-of-the-art generation quality (FID) on GOD dataset: <b>1.67</b> .
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    MinD-Vis
                </h3>
                <img src="./index_files/images/flowchart.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    Stage A (left): Self-supervised pre-training on large-scale fMRI dataset using Sparse-Coding based Masked Brain Modeling (SC-MBM);
                    Stage B (right): Double-Conditioned Latent Diffusion Model (DC-LDM) for image generation conditioned on brain recordings.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results compared with SOTA
                </h3>
                <img src="./index_files/images/compare_figs.png" class="img-responsive" alt="result with sota"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Generation Consistency
                </h3>
                <img src="./index_files/images/consistent.png" class="img-responsive" width="300" alt="consistency"><br>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Replication Dataset
                </h3>
                <img src="./index_files/images/bold5000.png" class="img-responsive" width="400" alt="bold5000"><br>
            </div>
        </div>

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@InProceedings{Xiang_2021_ICCV,
    author    = {Xiang, Tiange and Zhang, Chaoyi and Song, Yang and Yu, Jianhui and Cai, Weidong},
    title     = {Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {915-924}
}</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                The website template was borrowed from <a href="https://curvenet.github.io">Tiange Xiang</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
