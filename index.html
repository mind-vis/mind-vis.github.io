<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MinD-Vis</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding
            <br /><br />
            <!-- <small>
                CVPR 2021 (Oral presentation)
            </small>
            <br /><br /> -->
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        Zijiao Chen<sup>1*</sup>
                    </li>
                    <li>
                        Jiaxin Qing<sup>2*</sup>
                    </li>
                    <li>
                        Tiange Xiang<sup>3</sup>
                    </li>
                    <li>
                        Wan Lin Yue<sup>1</sup>
                    </li>
                     <li>
                        Juan (Helen) Zhou<sup>1</sup>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>National University of Singapore, Center for Sleep and Cognition
                    </li>
                    <br>
                    <li>
                        <sup>2</sup>The Chinese University of Hong Kong, Department of Information Engineering
                    </li>
                    <br>
                    <li>
                        <sup>3</sup>Standford University, Vision and Learning Lab
                    </li>
                    <li>
                        <sup>*</sup>Equal Contribution
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2105.01288" target='_blank'>
                        <img src="./index_files/images/paper.png" height="80px"><br>
                            <h4><strong>Main Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="./supplementary.pdf" target='_blank'>
                        <img src="./index_files/images/supp.png" height="80px"><br>
                            <h4><strong>Supp. Materials</strong></h4>
                        </a>
                    </li>
                     <li>
                        <a href="#dataset">
                        <img src="./index_files/images/data.png" height="80px"><br>
                            <h4><strong>Data</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/zjc062/mind-vis">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/first_fig.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    <i style="font-size:22px;">Motivation</i>
                    <br>
                    Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human vision and computer vision through the Brain-Computer Interface. 
                    However, due to the scarcity of data annotations and the complexity of underlying brain information, it is challenging to decode images with faithful details and meaningful semantics. 
                    <br>
                    <i style="font-size:22px;">Contribution</i>
                    <br>
                    In this work, we present <b>MinD-Vis</b>: Sparse <b>M</b>asked Bra<b>in</b> Modeling with <b>D</b>ouble<b>-</b>Conditioned Diffusion Model for <b>Vis</b>ion Decoding.                   
                    Specifically, by boosting the information capacity of representations learned in a large-scale resting-state fMRI dataset, we show that our MinD-Vis framework reconstructed <b>highly plausible images with semantically matching details</b> from brain recordings with very few training pairs. 
                    We benchmarked our model and our method outperformed state-of-the-arts in both <b>semantic mapping</b> (100-way semantic classification) and <b>generation quality</b> (FID) by <b>66%</b> and <b>41%</b>, respectively.
                    Exhaustive ablation studies are conducted to analyze our framework. 
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            A human visual decoding system that only reply on limited annotations.
                        </li>
                        <li>
                            State-of-the-art 100-way top-1 classification accuracy on GOD dataset: <b>23.9%</b>, outperforming the previous best by <b>66%</b>.
                        </li>
                        <li>
                            State-of-the-art generation quality (FID) on GOD dataset: <b>1.67</b>, outperforming the previous best by <b>41%</b>.
                        </li>
                        <li>
                            For the first time, we show that non-invasive brain recordings can be used to decode images with similar performance as invasive measures.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    MinD-Vis
                </h3>
                <img src="./index_files/images/flowchart.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    <b>Stage A</b> (left): Self-supervised pre-training on large-scale fMRI dataset using Sparse-Coding based Masked Brain Modeling <b>(SC-MBM)</b>;
                    <b>Stage B</b> (right): Double-Conditioned Latent Diffusion Model <b>(DC-LDM)</b> for image generation conditioned on brain recordings.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results compared with SOTA
                </h3>
                <img src="./index_files/images/compare_figs.png" class="img-responsive" alt="result with sota"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Generation Consistency &nbsp&nbsp&nbsp Replication Dataset
                </h3>
                <img src="./index_files/images/more_result.png" class="img-responsive" alt="consistency and bold5000"><br>
            </div>
        </div>


        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@InProceedings{Xiang_2021_ICCV,
    author    = {Xiang, Tiange and Zhang, Chaoyi and Song, Yang and Yu, Jianhui and Cai, Weidong},
    title     = {Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {915-924}
}</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                The website template was borrowed from <a href="https://curvenet.github.io">Tiange Xiang</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
